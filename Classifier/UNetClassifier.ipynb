{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers.core import Dense, Flatten, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from math import floor\n",
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from os.path import join as pJoin\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewSize = (160, 320)\n",
    "ModelDir = '/media/anubis/Data/OpenVINO/UnetFull/LAST'\n",
    "SegModelName = '0A_100E_160x320D.h5'\n",
    "ClassModelName = 'Class_100E_160x320D.h5'\n",
    "SegModelFile = pJoin(ModelDir, SegModelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNetFull(img_rows, img_cols, img_channels):  # 23 trainable layers, use same padding, en lugar de unpadding layers\n",
    "    x = Input(shape=(img_rows, img_cols, img_channels))\n",
    "    \n",
    "    # Encoder \n",
    "    conv1 = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    conv2 = Conv2D(64, (3, 3), padding='same', activation='relu')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(128, (3, 3), padding='same', activation='relu')(pool1)\n",
    "    conv4 = Conv2D(128, (3, 3), padding='same', activation='relu')(conv3)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    conv5 = Conv2D(256, (3, 3), padding='same', activation='relu')(pool2)\n",
    "    conv6 = Conv2D(256, (3, 3), padding='same', activation='relu')(conv5)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv6)\n",
    "    \n",
    "    conv7 = Conv2D(512, (3, 3), padding='same', activation='relu')(pool3)\n",
    "    conv8 = Conv2D(512, (3, 3), padding='same', activation='relu')(conv7)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv8)\n",
    "    \n",
    "    conv9 = Conv2D(1024, (3, 3), padding='same', activation='relu')(pool4)\n",
    "    conv10 = Conv2D(1024, (3, 3), padding='same', activation='relu')(conv9)\n",
    "    \n",
    "    # Decoder\n",
    "    convT1 = Conv2D(512, (2, 2), padding='same', activation='relu')(UpSampling2D(size=(2, 2))(conv10))\n",
    "    merge1 = concatenate([convT1, conv8], axis=3)\n",
    "    conv11 = Conv2D(512, (3, 3), padding='same', activation='relu')(merge1)\n",
    "    conv12 = Conv2D(512, (3, 3), padding='same', activation='relu')(conv11)\n",
    "    \n",
    "    convT2 = Conv2D(256, (2, 2), padding='same', activation='relu')(UpSampling2D(size=(2, 2))(conv12))\n",
    "    merge2 = concatenate([convT2, conv6], axis=3)\n",
    "    conv13 = Conv2D(256, (3, 3), padding='same', activation='relu')(merge2)\n",
    "    conv14 = Conv2D(256, (3, 3), padding='same', activation='relu')(conv13)\n",
    "    \n",
    "    convT3 = Conv2D(128, (2, 2), padding='same', activation='relu')(UpSampling2D(size=(2, 2))(conv14))\n",
    "    merge3 = concatenate([convT3, conv4], axis=3)\n",
    "    conv15 = Conv2D(128, (3, 3), padding='same', activation='relu')(merge3)\n",
    "    conv16 = Conv2D(128, (3, 3), padding='same', activation='relu')(conv15)\n",
    "    \n",
    "    convT4 = Conv2D(64, (2, 2), padding='same', activation='relu')(UpSampling2D(size=(2, 2))(conv16))\n",
    "    merge4 = concatenate([convT4, conv2], axis=3)\n",
    "    conv17 = Conv2D(64, (3, 3), padding='same', activation='relu')(merge4)\n",
    "    conv18 = Conv2D(64, (3, 3), padding='same', activation='relu')(conv17)\n",
    "\n",
    "    y = Conv2D(2, (1, 1), activation='softmax')(conv18)\n",
    "\n",
    "    return Model(inputs = x, outputs = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNetFull(NewSize[0], NewSize[1], 3)\n",
    "model.load_weights(SegModelFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.layers[-24].output\n",
    "predictions = Flatten()(x)\n",
    "predictions = Dense(1024, activation = 'relu')(predictions)\n",
    "predictions = Dropout(0.5)(predictions)\n",
    "predictions = Dense(512, activation = 'relu')(predictions)\n",
    "predictions = Dropout(0.5)(predictions)\n",
    "predictions = Dense(3, activation = 'softmax')(predictions)\n",
    "classifier = Model(inputs = model.inputs, outputs=predictions)\n",
    "\n",
    "for layer in classifier.layers[:-6]:\n",
    "        layer.trainable = False\n",
    "            \n",
    "print(classifier.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/media/anubis/Data/OpenVINO/Classification'\n",
    "TRAIN_DIR = pJoin(DATA_DIR, \"train\")\n",
    "VAL_DIR = pJoin(DATA_DIR, \"validation\")\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "train_datagen =  ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAIN_DIR, \n",
    "                                                    target_size = NewSize,\n",
    "                                                    class_mode = 'categorical',\n",
    "                                                    batch_size = BATCH_SIZE,\n",
    "                                                   )\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(VAL_DIR,\n",
    "                                                        target_size = NewSize, \n",
    "                                                        batch_size = floor(BATCH_SIZE),\n",
    "                                                        class_mode = 'categorical'\n",
    "                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "\n",
    "optimizer = SGD(lr = 0.009)\n",
    "\n",
    "classifier.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(pJoin(ModelDir, ClassModelName), monitor='val_loss',  verbose=0, save_best_only=True,\n",
    "                             mode='min', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = classifier.fit_generator(train_generator, epochs = NUM_EPOCHS,  \n",
    "                                       steps_per_epoch = len(train_generator), \n",
    "                                       shuffle = True,\n",
    "                                       callbacks = [checkpoint],\n",
    "                                       validation_data = validation_generator,\n",
    "                                       validation_steps = len(validation_generator))\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 4) # Hacer las figuras m√°s grandes\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "BestAcc = np.argmax(history.history['val_acc'])\n",
    "plt.plot(history.history['acc'], label='Training')\n",
    "plt.plot(history.history['val_acc'], label='Validation')\n",
    "plt.plot(BestAcc, history.history['acc'][BestAcc],'go')\n",
    "plt.plot(BestAcc, history.history['val_acc'][BestAcc],'go')\n",
    "plt.ylabel('Accuracy', fontsize = 20)\n",
    "plt.xlabel('Epoch', fontsize = 20)\n",
    "plt.suptitle('Learning curves', fontsize = 28)\n",
    "plt.subplot(1, 2, 2)\n",
    "BestLoss = np.argmin(history.history['val_loss'])\n",
    "plt.plot(history.history['loss'], label='Training')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.plot(BestLoss, history.history['val_loss'][BestLoss],'go')\n",
    "plt.plot(BestLoss, history.history['loss'][BestLoss],'go')\n",
    "plt.xlabel('Epoch', fontsize = 20)\n",
    "plt.ylabel('Loss', fontsize = 20)\n",
    "\n",
    "plt.savefig(pJoin(ModelDir, 'ACC_LOSS_' + ClassModelName + '.png'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load_weights(pJoin(ModelDir, 'Classifier.h5'))\n",
    "TrainScore = classifier.evaluate_generator(train_generator, steps=89)\n",
    "ValidationScore = classifier.evaluate_generator(validation_generator, steps=89)\n",
    "file = open(pJoin(ModelDir, 'BestClassifierAssess_' + ClassModelName + '.txt'), 'w') \n",
    "file.write('Training\\n')\n",
    "for Value in TrainScore:\n",
    "    file.write(str(Value) + '\\n') \n",
    "file.write('Validation\\n')\n",
    "for Value in ValidationScore:\n",
    "    file.write(str(Value) + '\\n')\n",
    "file.close()\n",
    "print('Model assess saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hist_df = pd.DataFrame(history.history) \n",
    "\n",
    "hist_csv_file = pJoin(ModelDir, 'ClassifierHistory_' + ClassModelName + '.csv')\n",
    "with open(hist_csv_file, mode = 'w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_class = randint(0, 3)\n",
    "img_dir = pJoin(DATA_DIR, str(img_class))\n",
    "img_names = listdir(img_dir)\n",
    "file_name = img_names[randint(0, len(img_names))]\n",
    "\n",
    "image_file1 = pJoin(img_dir, file_name)\n",
    "\n",
    "img_original = imread(image_file1)\n",
    "img = resize(img_original, NewSize, mode = 'constant')\n",
    "\n",
    "img1 = np.expand_dims(img, axis=0)\n",
    "img1.shape\n",
    "\n",
    "response = classifier.predict(img1)\n",
    "print(response)\n",
    "\n",
    "imshow(img)\n",
    "plt.title(file_name.split('.')[0] + '. Class: ' + str(img_class) + ' Diagnosis: ' + str(response.argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
